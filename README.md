# Code-Mixed Sentiment Analysis using HiEn-mBERT+

A lightweight transformer-based approach for **Hindiâ€“English (Hi-En) code-mixed sentiment analysis**, designed to handle noisy social media text with mixed scripts and transliteration.

## ğŸ“Œ Project Overview

Code-mixed text (e.g., Hindiâ€“English) is common on social media but challenging for NLP models due to spelling variations, transliteration, and language switching.  
This project proposes **HiEn-mBERT+**, a fine-tuned multilingual BERT-based model optimized for sentiment classification on Hindiâ€“English code-mixed data.

The work was carried out during a **Research Internship at IIIT Ranchi (May 2025 â€“ July 2025)**.

## ğŸ§  Key Contributions

- Developed a **lightweight BERT-based model** tailored for Hindiâ€“English code-mixed sentiment analysis  
- Designed preprocessing pipelines to handle:
  - Code-mixing and transliteration
  - Noisy social media text
- Fine-tuned multilingual transformer models for improved contextual understanding
- Conducted comparative evaluation with classical machine learning baselines

## ğŸ“Š Results of mBERT

| Metric        | Score |
|---------------|-------|
| Accuracy      | **84.8%** |
| F1-Score      | **84.5%** |

- The proposed model **outperformed classical ML baselines**
- Demonstrated better robustness on code-mixed text compared to traditional text-based approaches

## ğŸ› ï¸ Tech Stack

- **Python**
- **PyTorch**
- **Hugging Face Transformers**
- **mBERT / XLM-RoBERTa**
- **scikit-learn**
- **NumPy, Pandas**

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ clean_text.py
â”‚   â””â”€â”€ language_handling.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ hien_mbert_plus.py
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ results/
â”‚   â””â”€â”€ metrics.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
